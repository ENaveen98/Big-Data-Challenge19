{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Rules </h2>\n",
    "\n",
    "<li> Output should be in the specified format below </li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Background Summary</h3>\n",
    "\n",
    "An order driven market is a financial market where all buyers and sellers display the prices at which they wish to buy or sell a particular security, as well as the amounts of the security desired to be bought or sold. In these markets, participants may submit limit orders or market orders. \n",
    "\n",
    "In a limit order, you specify how much of the asset you want to buy or sell, and the price you want. If there are matching orders on the book (e.g. someone who wants to sell at the same price, or lower, as the price at which you want to buy), your order will be filled immediately. If not, your order will stay on the book until matching orders arrive (which could be never). It is also possible for a limit order to be only partially filled, if the counterparty wants to trade a smaller amount than you did. In that case the rest of the order remains on the book.\n",
    "\n",
    "In a market order, you only specify how much of the asset you want to trade. Your order is then filled immediately at the best price currently available on the market. For instance, if you place a market buy order, you will be matched with the current lowest-priced sell order on the book. If that order is not large enough to completely fill yours, the next-lowest sell order will be used to fill some more of yours, and so on.(You are encouraged to go through the 1st suggested reading for a pictorial understanding of order book and price dynamics)\n",
    "\n",
    "In this competition, we use tick data. Tick data refers to any market data which shows the price and volume of every print.  Additionally changes to the state of the order book occur in the form of trades and quotes. A quote event occurs whenever the best bid or the ask price is updated. A trade event takes place when shares are bought or sold.\n",
    "\n",
    "The aim of this competition is to determine the relationship between recent past order book events and future stock price for 30 seconds time-horizons. Few factors that are explored in the literature to predict price movements:  \n",
    "<li>Order arrival rate</li>\n",
    "<li>Bid-ask spread</li>\n",
    "<li>Order book imbalance</li>\n",
    "<li>Trade volume @ Bid price vs Trade volume @ Ask price</li>\n",
    "\n",
    "Certain factors, such as current order book imbalance, tend to have good predictive power for very short-time time-horizons (under 10-20 seconds), however other factors might be important for time-horizons of more than a minute.\n",
    "\n",
    "Equity markets are very fast and it is important to understand that multiple high-frequency events can occur in the same milliseconds. Analysing and understanding the data is critical before applying machine learning models.\n",
    "\n",
    "This problem is based on real-life problem we work on. Another important point to note - trade event and quote event timestamp will rarely be at the same-time, usually quote event time stamp is before trade event time stamp. Refer to examples in the <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.merge_asof.html\">link</a> on how you could join this data \n",
    "\n",
    "\n",
    "<h2> Suggested Reading </h2>\n",
    "\n",
    "Basic introduction of Limit Order Book\n",
    "<li><a href=\"https://journal.r-project.org/archive/2011/RJ-2011-010/RJ-2011-010.pdf\"> Analyzing an Electronic Limit Order Book </a></li>\n",
    "<li><a href=\"https://www.amazon.in/Algorithmic-Trading-DMA-Introduction-Strategies/dp/0956399207\">Algorithmic Trading and DMA</a></li>\n",
    "<li><a href=\"https://www.quantstart.com/articles/high-frequency-trading-ii-limit-order-book\">HFT - Limit Order Book</a></li>\n",
    "\n",
    "<br/>\n",
    "Advanced Topics \n",
    "<li><a href=\"http://www.personal.psu.edu/qxc2/research/jfuturesmarkets-2008.pdf\"> The Information Content of an Open Limit Order Book</a></li>\n",
    "<li><a href=\"http://eprints.maths.ox.ac.uk/1895/1/Darryl%20Shen%20%28for%20archive%29.pdf\"> Order Imbalance Based Strategy in High Frequency Trading</a></li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Problem Statement</h2>\n",
    "\n",
    "The aim of the problem is to develop a forecasting model to predict a stock's short-term price movement. The use of such prediction models is widely prevalent in algorithmic trading. Algorithmic trading, sometimes referred to as high-frequency trading in specific circumstances, is the use of automated systems to identify true(money making) signals among massive amounts of data that capture the underlying stock dynamics. These models can be leveraged to develop profitable trading strategies(akin to hedge funds) to help investors/traders achieve better returns. Contestants are expected and encouraged to think of empirical models/heuristics in order to better predict the price evolution of the hypothetical stock.\n",
    "\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Submission Instructions</h2>\n",
    "<br/>Algorithm/model should be developed without changing the order in the submission. Steps mentioned is the order of code execution.\n",
    "<ol>\n",
    "<li>Install all required libraries</li>\n",
    "<li>Parameters such as file names for in-sample data & out-sample data</li>\n",
    "<li>Algo-specific parameters</li>\n",
    "<li>Functions to load data & evaluate performance</li>\n",
    "<li>Train the model using in-sample data</li>\n",
    "    <ul>\n",
    "        <li>Trained model should be a <a href=\"https://docs.python.org/3/library/pickle.html\">pickle</a> or a function with values</li>\n",
    "    </ul>\n",
    "    \n",
    "<li>Predict with the trained model using out_sample data and evaluate the model performance</li>\n",
    "</ol>\n",
    "\n",
    "<b>Important Instructions</b>\n",
    "<ol>\n",
    "    <li>Cells that begin with \"#[DONOTCHANGE]\" shouldn't be changed</li>\n",
    "    <li>Submission will not be considered if the program fails to run</li>\n",
    "    <li>Tick frequency shouldn't be modified for out sample data</li>\n",
    "    <li>Only first-prediction will be considered for a mid-price until it changes [More detail below] </li>\n",
    "    <li>Model code should be commented</li>\n",
    "    <li>Brief description of the model (Preferably less than 1-page)</li>\n",
    "</ol>\n",
    "\n",
    "<br/><br/>\n",
    "<b> First-Prediction of every mid-price for evaluation </b>\n",
    "\n",
    "predMid is the predicted mid-price 30 seconds ahead. NA in predMid implies that the model doesn't have a prediction. Only valid predictions will be considered for evaluation.\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "date|sym|bsize|bid|ask|asize|mid|predMid|ValidPrediction\n",
    "-----|-----|-----|-----|-----|-----|-----|-----|-----\n",
    "2018.01.02D08:00:28.913000000|BATS.L|4816|5008|5011|569|5009.5|5020|Yes\n",
    "2018.01.02D08:00:28.913000000|BATS.L|3327|5008|5011|569|5009.5|5020|<b>No</b>\n",
    "2018.01.02D08:00:28.913000000|BATS.L|3327|5008|5015|616|5011.5|5018|Yes\n",
    "2018.01.02D08:00:28.917000000|BATS.L|3363|5008|5015|616|5011.5|NA|-\n",
    "2018.01.02D08:00:28.939000000|BATS.L|5045|5008|5015|616|5011.5|5018|<b>No</b>\n",
    "2018.01.02D08:00:28.939000000|BATS.L|5045|5008|5016|45|5012|5005|Yes\n",
    "2018.01.02D08:00:29.028000000|BATS.L|1718|5008|5016|45|5012|5005|<b>No</b>\n",
    "2018.01.02D08:00:29.052000000|BATS.L|1718|5008|5015|90|5011.5|NA|-\n",
    "2018.01.02D08:00:29.052000000|BATS.L|1718|5008|5015|256|5011.5|5020|Yes\n",
    "2018.01.02D08:00:29.052000000|BATS.L|1718|5008|5015|278|5011.5|5020|<b>No</b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data </h2>\n",
    "\n",
    "In-sample data\n",
    "<ul>\n",
    "<li>trade_in.csv</li>\n",
    "<li>quote_in.csv</li>\n",
    "</ul>\n",
    "Out-sample data\n",
    "<ul>\n",
    "<li>trade_out.csv</li>\n",
    "<li>quote_out.csv</li>\n",
    "</ul>\n",
    "\n",
    "Your model will be evaluated with a different set of date set.\n",
    "\n",
    "<b> Data Fields </b>\n",
    "    \n",
    "Variable Name|Description|Type|Example\n",
    "-----|-----|-----|-----\n",
    "datetime|Datetime of the event|Datetime in format yyyy.mm.ddDHH:MM:SS.fff|2018.02.10D10:20:20.100\n",
    "ric|Stock ticker|String|BP.L\n",
    "price|Last trade price|Double|3.45\n",
    "size|Last trade size|Integer|10000\n",
    "bid|Current Bid Price|Double|3.45\n",
    "ask|Current Ask Price|Double|3.5\n",
    "bsize|Current Bid Size|Integer|4000\n",
    "asize|Current Ask Size|Integer|5000\n",
    "mid|0.5 * (bid + ask)|Double|3.475\n",
    "predictedMid|Mid-price predicted by the model|Double|3.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\naveen 98\\anaconda3\\lib\\site-packages (0.22.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\naveen 98\\anaconda3\\lib\\site-packages (from pandas) (1.14.5)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\users\\naveen 98\\anaconda3\\lib\\site-packages (from pandas) (2016.4)\n",
      "Requirement already satisfied: python-dateutil>=2 in c:\\users\\naveen 98\\anaconda3\\lib\\site-packages (from pandas) (2.5.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\naveen 98\\anaconda3\\lib\\site-packages (from python-dateutil>=2->pandas) (1.10.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytz in c:\\users\\naveen 98\\anaconda3\\lib\\site-packages (2016.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\naveen 98\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\naveen 98\\anaconda3\\lib\\site-packages (from matplotlib) (1.14.5)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\naveen 98\\anaconda3\\lib\\site-packages (from matplotlib) (2.5.3)\n",
      "Requirement already satisfied: pytz in c:\\users\\naveen 98\\anaconda3\\lib\\site-packages (from matplotlib) (2016.4)\n",
      "Requirement already satisfied: cycler in c:\\users\\naveen 98\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,>=1.5.6 in c:\\users\\naveen 98\\anaconda3\\lib\\site-packages (from matplotlib) (2.1.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\naveen 98\\anaconda3\\lib\\site-packages (from python-dateutil->matplotlib) (1.10.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\naveen 98\\anaconda3\\lib\\site-packages (1.14.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\naveen 98\\anaconda3\\lib\\site-packages (0.19.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached https://files.pythonhosted.org/packages/63/90/46872c58db4a924b794921dc6790f426ffaaf19feca9b5023d396963f175/scikit_learn-0.20.2-cp35-cp35m-win_amd64.whl\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in c:\\users\\naveen 98\\anaconda3\\lib\\site-packages (from scikit-learn) (0.19.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in c:\\users\\naveen 98\\anaconda3\\lib\\site-packages (from scikit-learn) (1.14.5)\n",
      "Installing collected packages: scikit-learn\n",
      "  Found existing installation: scikit-learn 0.19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot uninstall 'scikit-learn'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\n",
      "You are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#1st step - Install all required libraries\n",
    "#IMPORTANT : Install necessary libraries if not already present.\n",
    "\n",
    "!pip install pandas\n",
    "!pip install pytz\n",
    "!pip install matplotlib\n",
    "!pip install numpy\n",
    "!pip install scipy\n",
    "!pip install -U scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#[DONOTCHANGE]\n",
    "#2nd step - all parameters\n",
    "class Parameters(object):\n",
    "    pass\n",
    "\n",
    "param = Parameters()\n",
    "param.tickSize = 0.5 #tick size is 0.5 GBp i.e. 0.005 GBP\n",
    "\n",
    "param.fileDirectory = './intraday/'\n",
    "param.trade_InSampleFile = 'trade_in.csv'\n",
    "param.quote_InSampleFile = 'quote_in.csv'\n",
    "\n",
    "param.trade_OutSampleFile = 'trade_out.csv'\n",
    "param.quote_OutSampleFile = 'quote_out.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3rd step - Model specific parameters\n",
    "#param.imbalanceThreshold = 0.7\n",
    "#param.timeDuration = 30 #30 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#[DONOTCHANGE]\n",
    "#4th step - Functions to load data & evaluate performance\n",
    "\n",
    "#Initialise libraries and functions\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Disable certain warnings\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "#Identify future mid prices - 30 seconds duration\n",
    "def IdentifyFutureMidPrices(df, predictionDuration = 30):\n",
    "    futDat = df[['datetime', 'mid']].rename(columns={'mid':'futMid'})\n",
    "    futDat['datetime'] = futDat['datetime'] - pd.offsets.timedelta(seconds=int(predictionDuration))\n",
    "    return pd.merge_asof(df, futDat, on='datetime', direction='backward')\n",
    "\n",
    "def ReadCSV(file):\n",
    "    print('Loading file - ' + file)\n",
    "    df = pd.read_csv(file)\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], format=\"%Y-%m-%dD%H:%M:%S.%f\")\n",
    "    return df\n",
    "\n",
    "#Load data\n",
    "def LoadData(path, tradeFile, quoteFile):\n",
    "    tradeFile = os.path.join(path, tradeFile)\n",
    "    quoteFile = os.path.join(path, quoteFile)\n",
    "\n",
    "    trade_df = ReadCSV(tradeFile)\n",
    "    quote_df = ReadCSV(quoteFile)\n",
    "    \n",
    "    quote_df['mid'] = 0.5*(quote_df['bid'].copy() + quote_df['ask'].copy())\n",
    "    quote_df['midChangeGroup'] = quote_df['mid'].diff().ne(0).cumsum()\n",
    "    quote_df = IdentifyFutureMidPrices(quote_df)\n",
    "    return trade_df, quote_df\n",
    "\n",
    "#Evaluation function\n",
    "#df should contain columns - datetime, sym, bsize, bid, ask, asize, predMid (model predicted mid-price)\n",
    "#Function to evaluate results\n",
    "def RMS(df):\n",
    "    df = df.groupby(['midChangeGroup']).first().reset_index()\n",
    "    tmp = df.dropna(subset=['predMid', 'futMid'])\n",
    "    rms = sqrt(mean_squared_error(tmp['futMid'], tmp['predMid']))\n",
    "    predCount = len(tmp['predMid'])\n",
    "    print('RMS = %.4f. #Predictions = %s' % (rms, predCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#5th Step\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as prf\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostRegressor, BaggingRegressor, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier, ExtraTreeRegressor\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "def my_Feature_Engineering(quote_df, trade_df):\n",
    "    print('Starting myPred_1')\n",
    "\n",
    "    df = pd.merge_asof(quote_df, trade_df, on='datetime') #try merging the other way (This seems correct though)\n",
    "    print('Shape before: ', df.shape)\n",
    "    df = df.set_index('datetime',drop=False)\n",
    "    df['midma120'] = df['mid'].rolling('120s').mean()\n",
    "    df['midma90'] = df['mid'].rolling('90s').mean()\n",
    "    df['midma60'] = df['mid'].rolling('60s').mean()\n",
    "    df['midma30'] = df['mid'].rolling('30s').mean()\n",
    "    df['middiff1'] = df['midma30'] - df['midma90']\n",
    "    df['askma120'] = df['ask'].rolling('120s').mean()\n",
    "    df['askma90'] = df['ask'].rolling('90s').mean()\n",
    "    df['askma60'] = df['ask'].rolling('60s').mean()\n",
    "    df['askma30'] = df['ask'].rolling('30s').mean()\n",
    "    df['askdiff1'] = df['askma30'] - df['askma90']\n",
    "    df['asksizema120'] = df['asize'].rolling('120s').mean()\n",
    "    df['asksizema90'] = df['asize'].rolling('90s').mean()\n",
    "    df['asksizema60'] = df['asize'].rolling('60s').mean()\n",
    "    df['asksizema30'] = df['asize'].rolling('30s').mean()\n",
    "    df['asksizediff1'] = df['asksizema30'] - df['asksizema90']\n",
    "    df['bidma120'] = df['bid'].rolling('120s').mean()\n",
    "    df['bidma90'] = df['bid'].rolling('90s').mean()\n",
    "    df['bidma60'] = df['bid'].rolling('60s').mean()\n",
    "    df['bidma30'] = df['bid'].rolling('30s').mean()\n",
    "    df['biddiff1'] = df['bidma30'] - df['bidma90']\n",
    "    df['bidsizema120'] = df['bsize'].rolling('120s').mean()\n",
    "    df['bidsizema90'] = df['bsize'].rolling('90s').mean()\n",
    "    df['bidsizema60'] = df['bsize'].rolling('60s').mean()\n",
    "    df['bidsizema30'] = df['bsize'].rolling('30s').mean()\n",
    "    df['bidsizediff1'] = df['bidsizema30'] - df['bidsizema90']\n",
    "    #dftemp = pd.DataFrame()\n",
    "    df['length'] = df['midChangeGroup'].map(df['midChangeGroup'].value_counts())\n",
    "    df = df.dropna(subset=['price', 'size', 'futMid']).reset_index(drop=True)\n",
    "    df.drop(columns = ['sym_y'], inplace = True)\n",
    "    df['time'] = df['datetime'].dt.time\n",
    "    df['dayofweek'] = df['datetime'].dt.dayofweek\n",
    "    df['date'] = pd.to_datetime(df['datetime'].dt.date)\n",
    "    df['diff'] = df['datetime']-df['date']\n",
    "    df['seconds'] = df['diff'].dt.seconds\n",
    "    df = df.groupby(['midChangeGroup']).last().reset_index()\n",
    "    df.drop(columns = ['diff', 'time', 'date'], inplace = True)\n",
    "\n",
    "    print('Shape after: ', df.shape)\n",
    "    print('Finished myPred_1')\n",
    "\n",
    "    print('Starting myPred_2')\n",
    "    print('Shape before: ', df.shape)\n",
    "    #we have lost datetime now. Do not drop that in myPred_1 if required later.\n",
    "    #df['seconds'].min()=28808 for IN data\n",
    "    df['seconds'] = df['seconds'] - 28808 + 1\n",
    "\n",
    "    df['futMidLabels'] = df['futMid'] - df['mid']\n",
    "    #Create Labels 1 if futMid value increases from Mid and -1 if futMid value decreases from Mid\n",
    "    a = np.array(df['futMidLabels'].values.tolist())\n",
    "    df['futMidLabels'] = np.where(a > 0, 1, a).tolist()\n",
    "    a = np.array(df['futMidLabels'].values.tolist())\n",
    "    df['futMidLabels'] = np.where(a < 0, -1, a).tolist()\n",
    "    \n",
    "    df['futMidLabels'] = df['futMidLabels'].astype('int32')\n",
    "    df.drop(columns = ['sym_x'], inplace = True)\n",
    "    df = df.drop(df[(df['size']>250)].index)\n",
    "    df['imbalance'] = (df['bsize']-df['asize'])/(df['bsize']+df['asize'])\n",
    "    df['Shareimbalance'] = (df['bsize'])/(df['bsize']+df['asize'])\n",
    "\n",
    "    print('Shape after: ', df.shape)\n",
    "    print('Finished myPred_1')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def out_1(quote_df, trade_df, load=False, save_IN=False, save_OUT=False):\n",
    "\n",
    "    if load == True :\n",
    "        print('Loading...')\n",
    "        if save_IN == True:\n",
    "            df_out_2 = pd.read_csv('df_out_2_IN.csv')\n",
    "        if save_OUT == True:\n",
    "            df_out_2 = pd.read_csv('df_out_2_OUT.csv')\n",
    "    else:\n",
    "        df_out_2 = my_Feature_Engineering(quote_df, trade_df)\n",
    "        if save_IN == True :\n",
    "            df_out_2.to_csv('df_out_2_IN.csv')\n",
    "        if save_OUT == True :\n",
    "            df_out_2.to_csv('df_out_2_OUT.csv')\n",
    "            \n",
    "    return df_out_2\n",
    "\n",
    "def features_labels_split_classification(features):\n",
    "    \n",
    "    features['futMidLabels'] = features['futMidLabels'].astype('int32')\n",
    "    labels = np.array(features['futMidLabels'])\n",
    "    features= features.drop(['futMidLabels','futMid', 'midChangeGroup', 'datetime'], axis = 1)\n",
    "    feature_list = list(features.columns)\n",
    "    features = np.array(features)\n",
    "\n",
    "    print('Training Features Shape:', features.shape)\n",
    "    print('Training Labels Shape:', labels.shape)\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "def fit_Classification_Model(train_features, train_labels):\n",
    "    rfc = BaggingClassifier(base_estimator = RandomForestClassifier(n_estimators = 250, random_state = 42, max_depth = 8,  n_jobs=3, verbose=2), verbose=2, n_jobs=2,  n_estimators=4, random_state=42)\n",
    "    rfc.fit(train_features, train_labels)\n",
    "    return rfc\n",
    "\n",
    "def print_classification_inference(rfc, test_features, test_labels):\n",
    "    \n",
    "    predictions = rfc.predict(test_features)\n",
    "    print('Groud Truth Spread:', Counter(test_labels))\n",
    "    print('Prediction Spread:', Counter(predictions))\n",
    "    dict = {'predict':predictions, 'labels':test_labels}\n",
    "    columns = ['predict', 'labels']\n",
    "    df = pd.DataFrame(dict, columns=columns)\n",
    " \n",
    "    #df.to_csv('predict_label_Out.csv')\n",
    "    \n",
    "    tmp = df.copy()\n",
    "    tmp['labels'] = tmp['labels'].astype('int32')\n",
    "    tmp['predict'] = tmp['predict'].astype('int32')\n",
    "    tmp['errsq'] = (tmp['labels']-tmp['predict'])**2 \n",
    "    \n",
    "    #Changing values 1.0 and 4.0 to 4.0 except 0.\n",
    "    tmp['errsq'] = tmp['errsq'].map({1.0: 4.0, 4.0: 4.0, 0.0:0.0})\n",
    "\n",
    "    misclassified_number = np.sum((tmp['errsq'].values)/4)\n",
    "    predCount = len(tmp['labels'])\n",
    "    total_classified = predCount\n",
    "    misclassified_fraction = misclassified_number/total_classified\n",
    "\n",
    "    print('misclassified_number:', misclassified_number)\n",
    "    print('total_classified:', total_classified)\n",
    "    print('misclassified_fraction:', misclassified_fraction)\n",
    "    \n",
    "    x = prf(tmp['labels'], tmp['predict'])\n",
    "\n",
    "    print('precision: ', x[0])\n",
    "    print('recall: ', x[1])\n",
    "    print('f_beta: ', x[2])\n",
    "    #print('support: ', x[3])\n",
    "    print('labels:', [-1, 0, 1])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def features_labels_split_regression(df_predict, features):\n",
    "    features['tickpredict'] = df_predict['predict']\n",
    "    labels = np.array(features['futMid']-features['mid'])\n",
    "    features2 = features[['bsize','bid', 'ask', 'asize', 'mid','middiff1','askdiff1','biddiff1',\n",
    "                          'asksizediff1','bidsizediff1','midma120', 'midma60','midma30','askma120',\n",
    "                          'askma60','askma30','bidma120', 'bidma60','bidma30','bidsizema120', 'bidsizema60',\n",
    "                          'bidsizema30','asksizema120', 'asksizema60','asksizema30','price', 'size', 'dayofweek',\n",
    "                          'seconds', 'imbalance', 'Shareimbalance', 'tickpredict']]\n",
    "    feature_list = list(features2.columns)\n",
    "    features2 = np.array(features2)\n",
    "    \n",
    "    print('Training Features Shape:', features2.shape)\n",
    "    print('Training Labels Shape:', labels.shape)\n",
    "    \n",
    "    return features2, labels\n",
    "\n",
    "def fit_Regression_model(train_features, train_labels) :\n",
    "    #mlp = BaggingRegressor(base_estimator = ExtraTreeRegressor(max_depth=3,random_state=42), n_jobs=2,  n_estimators=8, random_state=42) #1n MAXIMUM\n",
    "    #mlp = BaggingRegressor(base_estimator = ExtraTreeRegressor(max_depth=3), n_jobs=2,  n_estimators=15)#2n MAXIMUM\n",
    "    mlp = BaggingRegressor(base_estimator = ExtraTreeRegressor(max_depth=6,random_state=42), n_jobs=2,  n_estimators=10, random_state=42)\n",
    "    mlp.fit(train_features, train_labels)\n",
    "    \n",
    "    return mlp\n",
    "\n",
    "def combine_pred_with_data_deltachange(predict, features):\n",
    "    dict2 = {'predict':predict}\n",
    "    columns = ['predict']\n",
    "    df = pd.DataFrame(dict2, columns=columns)\n",
    "    dfcopy = features[['datetime', 'midChangeGroup', 'seconds', 'mid', 'futMid']]\n",
    "    dfcopy['predMid'] = dfcopy['mid'] + df['predict']\n",
    "    dfcopy['diff_Mid_futMidPred'] = (dfcopy['predMid']-dfcopy['mid'])**2\n",
    "    dfcopy['diff_Mid_futMidPred'] = dfcopy['diff_Mid_futMidPred']**0.5\n",
    "    #dfcopy = dfcopy.drop(dfcopy[(dfcopy['diff_Mid_futMidPred']>5)].index)\n",
    "    dfcopy['diff_futMid_futMidPred'] = (dfcopy['predMid'] - dfcopy['futMid'])**2\n",
    "    #Drop predictions at Unstable times determined from Visualizing predictions for IN Data\n",
    "    dfcopy = dfcopy.drop(dfcopy[(dfcopy['seconds']<10000)].index)\n",
    "    dfcopy = dfcopy.drop(dfcopy[(dfcopy['seconds']>19500)].index)\n",
    "    rms = dfcopy['diff_futMid_futMidPred'].mean()\n",
    "    rms = rms**0.5\n",
    "    print('rmse',rms)\n",
    "    dfcopy.drop(columns = ['diff_futMid_futMidPred', 'diff_Mid_futMidPred'], inplace = True)\n",
    "    \n",
    "    return dfcopy\n",
    "\n",
    "\n",
    "print('\\n------- Logging Process Steps - START (can ignore)-------\\n')\n",
    "\n",
    "print('Training Process [1/8]')\n",
    "dirContents = dir()\n",
    "if not ('tradeIndf' in dirContents and 'quoteIndf' in dirContents):\n",
    "    tradeIndf, quoteIndf = LoadData(param.fileDirectory, param.trade_InSampleFile, param.quote_InSampleFile)\n",
    "print('Done....')\n",
    "    \n",
    "print('\\nTraining Process [2/8]')\n",
    "load = False #load is True as the features for IN data have been feature engineered beforehand and can be loaded.\n",
    "if load==True :\n",
    "    features_IN = out_1(quoteIndf, tradeIndf, load=True, save_IN=True, save_OUT=False)\n",
    "    features_IN.drop(columns = ['Unnamed: 0'], inplace=True)\n",
    "else:\n",
    "    features_IN = out_1(quoteIndf, tradeIndf, load=False, save_IN=True, save_OUT=False)\n",
    "    features_IN = out_1(quoteIndf, tradeIndf, load=True, save_IN=True, save_OUT=False)\n",
    "    features_IN.drop(columns = ['Unnamed: 0'], inplace=True)\n",
    "    print('features were calculated')\n",
    "print('features_IN Shape after Feature Engineering: ', features_IN.shape)\n",
    "print('Done....')\n",
    "\n",
    "print('\\nTraining Process [3/8]')\n",
    "train_features_IN, train_labels_IN = features_labels_split_classification(features_IN)\n",
    "print('Done....')\n",
    "\n",
    "print('\\nTraining Process [4/8]')\n",
    "rfc = fit_Classification_Model(train_features_IN, train_labels_IN)\n",
    "pickle.dump(rfc, open('bagging_rfc.sav', 'wb'))\n",
    "#print('Loading pickled model..')\n",
    "#rfc = pickle.load(open('bagging_rfc.sav', 'rb'))\n",
    "print('Done....')\n",
    "\n",
    "print('\\nTraining Process [5/8]')\n",
    "df_predict_label_IN = print_classification_inference(rfc, train_features_IN, train_labels_IN)\n",
    "print('Done....')\n",
    "\n",
    "\n",
    "print('\\nTraining Process [6/8]')\n",
    "mlp_train_features_IN, mlp_train_labels_IN = features_labels_split_regression(df_predict_label_IN, features_IN)\n",
    "print('Done....')\n",
    "\n",
    "print('\\nTraining Process [7/8]')\n",
    "mlp = fit_Regression_model(mlp_train_features_IN, mlp_train_labels_IN)\n",
    "pickle.dump(mlp, open('bagging_etr.sav', 'wb'))\n",
    "#print('Loading pickled model..')\n",
    "#mlp = pickle.load(open('bagging_etr.sav', 'rb'))\n",
    "print('Done....')\n",
    "\n",
    "print('\\nTraining Process [8/8]')\n",
    "predict_IN = mlp.predict(mlp_train_features_IN)\n",
    "Final_IN = combine_pred_with_data_deltachange(predict_IN, features_IN)\n",
    "Final_IN.head()\n",
    "Final_IN.to_csv('Final_IN.csv')\n",
    "print('Done....')\n",
    "\n",
    "print('\\n------- Logging Process Steps - END (can ignore)-------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------- Logging Process Steps - START (can ignore)-------\n",
      "\n",
      "Training Process [1/8]\n",
      "Done....\n",
      "\n",
      "Training Process [2/8]\n",
      "Starting myPred_1\n",
      "Shape before:  (1672797, 12)\n",
      "Shape after:  (370696, 39)\n",
      "Finished myPred_1\n",
      "Starting myPred_2\n",
      "Shape before:  (370696, 39)\n",
      "Shape after:  (350333, 41)\n",
      "Finished myPred_1\n",
      "Loading...\n",
      "features were calculated\n",
      "features_IN Shape after Feature Engineering:  (350333, 41)\n",
      "Done....\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Process [3/8]\n",
      "Training Features Shape: (350333, 37)\n",
      "Training Labels Shape: (350333,)\n",
      "Done....\n",
      "\n",
      "Training Process [4/8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:  9.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done....\n",
      "\n",
      "Training Process [5/8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:   25.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groud Truth Spread: Counter({-1: 165061, 1: 163271, 0: 22001})\n",
      "Prediction Spread: Counter({-1: 178482, 1: 171851})\n",
      "misclassified_number: 146831.0\n",
      "total_classified: 350333\n",
      "misclassified_fraction: 0.4191183816540263\n",
      "precision:  [0.58053473 0.         0.58124189]\n",
      "recall:  [0.62773762 0.         0.61178654]\n",
      "f_beta:  [0.60321415 0.         0.5961232 ]\n",
      "labels: [-1, 0, 1]\n",
      "Done....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Naveen 98\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Process [6/8]\n",
      "Training Features Shape: (350333, 32)\n",
      "Training Labels Shape: (350333,)\n",
      "Done....\n",
      "\n",
      "Training Process [7/8]\n",
      "Done....\n",
      "\n",
      "Training Process [8/8]\n",
      "rmse 1.5718962725588526\n",
      "Done....\n",
      "\n",
      "------- Logging Process Steps - END (can ignore)-------\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Logging Process Steps - START (can ignore)-------\n",
      "\n",
      "This step will take some time!\n",
      "Starting myPred_1\n",
      "Shape before:  (207297, 12)\n",
      "Shape after:  (53187, 39)\n",
      "Finished myPred_1\n",
      "Starting myPred_2\n",
      "Shape before:  (53187, 39)\n",
      "Shape after:  (50749, 41)\n",
      "Finished myPred_1\n",
      "Loading...\n",
      "features were calculated\n",
      "features_OUT Shape after feature engineering:  (50749, 41)\n",
      "Training Features Shape: (50749, 37)\n",
      "Training Labels Shape: (50749,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    5.1s finished\n",
      "C:\\Users\\Naveen 98\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groud Truth Spread: Counter({-1: 25065, 1: 22654, 0: 3030})\n",
      "Prediction Spread: Counter({-1: 25697, 1: 25052})\n",
      "misclassified_number: 24917.0\n",
      "total_classified: 50749\n",
      "misclassified_fraction: 0.49098504404027665\n",
      "precision:  [0.53068452 0.         0.48678748]\n",
      "recall:  [0.54406543 0.         0.53831553]\n",
      "f_beta:  [0.53729167 0.         0.51125645]\n",
      "labels: [-1, 0, 1]\n",
      "Training Features Shape: (50749, 32)\n",
      "Training Labels Shape: (50749,)\n",
      "rmse 1.4883918513233985\n",
      "\n",
      "------- Logging Process Steps - END (can ignore)-------\n",
      "\n",
      "Out-sample prediction\n",
      "RMS = 1.4884. #Predictions = 7322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"#Any required calculations\\ndef OutSamplePrediction(quote_df, trade_df): \\n    print('Out-sample prediction')    \\n    return InSamplePredictionModel(quote_df, trade_df)\\n    \\n    \\n#res = OutSamplePrediction(quoteOutdf, tradeOutdf)\\nRMS(res)\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6th step\n",
    "#Predict with the trained model using out_sample data\n",
    "\n",
    "#Load the out-sample csv if not in memory\n",
    "#Do not change tick frequency for outsample dataframe\n",
    "\n",
    "dirContents = dir()\n",
    "if not ('tradeOutdf' in dirContents and 'quoteOutdf' in dirContents):\n",
    "    tradeOutdf, quoteOutdf = LoadData(param.fileDirectory, param.trade_OutSampleFile, param.quote_OutSampleFile)\n",
    "\n",
    "print('------- Logging Process Steps - START (can ignore)-------\\n')\n",
    "print('This step will take some time!')\n",
    "def OutSamplePrediction(quote_df, trade_df):\n",
    "    load=False #load is False if the data is used for prediction and has not been feature engineered\n",
    "    if load == True :\n",
    "        features_OUT = out_1(quote_df, trade_df, load=True, save_IN=False, save_OUT=True)\n",
    "        features_OUT.drop(columns = ['Unnamed: 0'], inplace=True)\n",
    "    else :\n",
    "        features_OUT = out_1(quote_df, trade_df, load=False, save_IN=False, save_OUT=True)\n",
    "        features_OUT = out_1(quote_df, trade_df, load=True, save_IN=False, save_OUT=True)\n",
    "        features_OUT.drop(columns = ['Unnamed: 0'], inplace=True)\n",
    "        print('features were calculated')\n",
    "    print('features_OUT Shape after feature engineering: ', features_OUT.shape)    \n",
    "\n",
    "    train_features_OUT, train_labels_OUT = features_labels_split_classification(features_OUT)\n",
    "\n",
    "    df_predict_label_OUT = print_classification_inference(rfc, train_features_OUT, train_labels_OUT)\n",
    "\n",
    "    mlp_train_features_OUT, mlp_train_labels_OUT = features_labels_split_regression(df_predict_label_OUT, features_OUT)\n",
    "\n",
    "    predict_OUT = mlp.predict(mlp_train_features_OUT)\n",
    "    Final_OUT = combine_pred_with_data_deltachange(predict_OUT, features_OUT)\n",
    "    Final_OUT.head()\n",
    "    Final_OUT.to_csv('Final_OUT.csv')\n",
    "    return Final_OUT\n",
    "\n",
    "Final_OUT = OutSamplePrediction(quoteOutdf, tradeOutdf)\n",
    "print('\\n------- Logging Process Steps - END (can ignore)-------\\n')\n",
    "print('Out-sample prediction')\n",
    "RMS(Final_OUT)\n",
    "\n",
    "#IMPORTANT: If need to predict using new data it should be in the same format as the data that was given initially. ( i.e. quote_out, trade_out csv files)\n",
    "\n",
    "\n",
    "'''#Any required calculations\n",
    "def OutSamplePrediction(quote_df, trade_df): \n",
    "    print('Out-sample prediction')    \n",
    "    return InSamplePredictionModel(quote_df, trade_df)\n",
    "    \n",
    "    \n",
    "#res = OutSamplePrediction(quoteOutdf, tradeOutdf)\n",
    "RMS(res)'''"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
